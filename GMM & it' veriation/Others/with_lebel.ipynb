{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Load pre-extracted MNIST features and labels\n",
    "features_path = 'imbalanced_train_features.pt'\n",
    "labels_path = 'imbalanced_train_labels.pt'\n",
    "features = torch.load(features_path).numpy()\n",
    "labels = torch.load(labels_path).numpy()\n",
    "\n",
    "# Define majority and minority classes\n",
    "majority_class = 0\n",
    "minority_indices = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(\"Majority class:\", majority_class)\n",
    "print(\"Minority classes:\", minority_indices)\n",
    "\n",
    "# Step 2: Perform Fuzzy C-Means Clustering\n",
    "n_clusters = 10\n",
    "cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(features.T, n_clusters, 2, error=0.005, maxiter=1000, init=None)\n",
    "\n",
    "# Print centroids\n",
    "print(\"Centroids:\\n\", cntr)\n",
    "\n",
    "# Plot the data and centroids\n",
    "plt.scatter(features[:, 0], features[:, 1], c='blue', marker='o', alpha=0.5)\n",
    "plt.scatter(cntr[:, 0], cntr[:, 1], c='red', marker='x')\n",
    "plt.title('Fuzzy C-Means Clustering')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Responsibility matrix is initialized (done during FCM)\n",
    "\n",
    "# Step 4: Initialize GMM Parameters\n",
    "def initialize_gmm_parameters(data, cntr, n_clusters):\n",
    "    gmm = GaussianMixture(n_components=n_clusters, means_init=cntr)\n",
    "    gmm.fit(data)\n",
    "    return gmm\n",
    "\n",
    "gmm = initialize_gmm_parameters(features, cntr, n_clusters)\n",
    "mu = gmm.means_\n",
    "sigma = gmm.covariances_\n",
    "\n",
    "print(\"Initial GMM parameters:\")\n",
    "print(\"Means:\\n\", mu)\n",
    "print(\"Covariances:\\n\", sigma)\n",
    "\n",
    "# Step 5: Update means, covariance, and mixing coefficients (handled internally by GMM fitting)\n",
    "\n",
    "# Step 6: Select all the 9 minority components (those with lower mixing coefficients)\n",
    "print(\"Minority components indices:\", minority_indices)\n",
    "\n",
    "# Step 7: Calculate m_comp and apply GMM for each minority cluster\n",
    "def apply_gmm_to_minority(data, majority_data, minority_indices, labels):\n",
    "    gmm_params = {}\n",
    "    for index in minority_indices:\n",
    "        minority_data = data[labels == index]\n",
    "        if len(minority_data) < 2:  # Check if there are at least two samples\n",
    "            continue\n",
    "        m_comp = len(majority_data) / len(minority_data)\n",
    "        n_components = min(len(minority_data), max(1, int(np.round(m_comp))))  # Ensure n_components is at least 1 and <= number of samples\n",
    "        gmm = GaussianMixture(n_components=n_components, init_params='kmeans')\n",
    "        gmm.fit(minority_data)\n",
    "        gmm_params[index] = (gmm.means_, gmm.covariances_, m_comp)\n",
    "    return gmm_params\n",
    "\n",
    "majority_class_indices = labels == majority_class\n",
    "majority_data = features[majority_class_indices]\n",
    "gmm_params = apply_gmm_to_minority(features, majority_data, minority_indices, labels)\n",
    "\n",
    "# Step 8: Find k nearest neighbors among means for each element in minority classes\n",
    "def find_k_nearest_neighbours(data, means, k):\n",
    "    distances = cdist(data, means)\n",
    "    k_nearest_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "    k_nearest_means = means[k_nearest_indices]\n",
    "    return k_nearest_means\n",
    "\n",
    "k = 3\n",
    "\n",
    "# Step 9: Calculate mux and sigmax, and generate new elements\n",
    "def generate_new_elements(minority_data, k_nearest_means, sigma, k, n_c):\n",
    "    new_elements = []\n",
    "    for x in minority_data:\n",
    "        k_nearest_means = k_nearest_means.reshape(-1, k_nearest_means.shape[-1])  # Ensure k_nearest_means is 2D\n",
    "        mux = (np.sum(k_nearest_means, axis=0) + x) / (k + 1)\n",
    "        sigmax = (np.sum(sigma, axis=0) + np.cov(minority_data.T)) / (k + 1)\n",
    "        new_elements.append(np.random.multivariate_normal(mux, sigmax, n_c))\n",
    "    return np.vstack(new_elements)\n",
    "\n",
    "new_elements = []\n",
    "\n",
    "for index in minority_indices:\n",
    "    minority_data = features[labels == index]\n",
    "    if len(minority_data) < 2:  # Check if there are at least two samples\n",
    "        continue\n",
    "    means, covariances, m_comp = gmm_params[index]\n",
    "    if m_comp >= k:\n",
    "        k_nearest_means = find_k_nearest_neighbours(minority_data, means, k)\n",
    "        n_c = max(1, (len(majority_data) - len(minority_data)) // len(minority_data))\n",
    "        new_elements_class = generate_new_elements(minority_data, k_nearest_means, covariances, k, n_c)\n",
    "    else:\n",
    "        n_comp = min(len(minority_data), max(1, len(majority_data) // len(minority_data)))  # Ensure n_comp is at least 1 and <= number of samples\n",
    "        gmm = GaussianMixture(n_components=n_comp, init_params='kmeans')\n",
    "        gmm.fit(minority_data)\n",
    "        means, covariances = gmm.means_, gmm.covariances_\n",
    "        new_elements_class = []\n",
    "        n_c = max(1, len(minority_data) * (len(majority_data) // n_comp) // len(minority_data))  # Ensure n_c is at least 1\n",
    "        for mean, cov in zip(means, covariances):\n",
    "            new_elements_class.append(np.random.multivariate_normal(mean, cov, n_c))\n",
    "        new_elements_class = np.vstack(new_elements_class)\n",
    "\n",
    "    new_elements.append(new_elements_class)\n",
    "\n",
    "new_elements = np.vstack(new_elements)\n",
    "\n",
    "# Plotting original data and new generated elements\n",
    "plt.scatter(features[:, 0], features[:, 1], c='blue', marker='o', alpha=0.5, label='Original Data')\n",
    "plt.scatter(new_elements[:, 0], new_elements[:, 1], c='green', marker='s', alpha=0.5, label='Generated Data')\n",
    "plt.title('Original and Generated Data Points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
