{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmiL8kJtqlVk"
      },
      "source": [
        "# Class Imbalancing in Multilayer perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will:\n",
        "\n",
        "1.Load the CIFAR-10 dataset. \\\n",
        "2.Create an imbalanced training set.\\\n",
        "3.Apply SMOTE to balance the dataset.\\\n",
        "4.Train a CNN on the CIFAR-10 dataset.\\\n",
        "5.Extract features from the output layer of the trained CNN.\\\n",
        "6.Train an MLP using these extracted features.\\\n",
        "7.Test the MLP on the CIFAR-10 test dataset and print the accuracy."
      ],
      "metadata": {
        "id": "WaajW3Z5yNQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aVEHQXuQqlVl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCip92eMqlVm"
      },
      "source": [
        "### Step 1: Load CIFAR 10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2ARNLGgqlVn",
        "outputId": "73d55c4b-fcb0-4d2a-e018-f337c988006f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 84772974.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "train_dataset = datasets.CIFAR10('data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10('data', train=False, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzMjGBRGqlVn"
      },
      "source": [
        "### Step 2: Apply imbalancing function for class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "efNVZdL3qlVn"
      },
      "outputs": [],
      "source": [
        "# Function to create an imbalanced dataset\n",
        "def get_imbalanced_data(dataset, num_sample_per_class, shuffle=False, random_seed=0):\n",
        "    \"\"\"\n",
        "    Return a list of imbalanced indices from a dataset.\n",
        "    Input: A dataset (e.g., CIFAR-10), num_sample_per_class: list of integers\n",
        "    Output: imbalanced_list\n",
        "    \"\"\"\n",
        "    length = len(dataset)\n",
        "    num_sample_per_class = list(num_sample_per_class)\n",
        "    selected_list = []\n",
        "    indices = list(range(0, length))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(0, length):\n",
        "        index = indices[i]\n",
        "        _, label = dataset[index]\n",
        "        if num_sample_per_class[label] > 0:\n",
        "            selected_list.append(index)\n",
        "            num_sample_per_class[label] -= 1\n",
        "    return selected_list\n",
        "\n",
        "# Create imbalance in the dataset\n",
        "num_samples = [5000, 3000, 2000, 1000, 500, 200, 100, 50, 20, 10]\n",
        "imbalanced_indices = get_imbalanced_data(train_dataset, num_samples)\n",
        "train_dataset = Subset(train_dataset, imbalanced_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vSPuJqvqlVn"
      },
      "source": [
        "### Step 3: Apply SMOTE for balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nlZNsG7jqlVn"
      },
      "outputs": [],
      "source": [
        "# Note: SMOTE is usually applied on feature vectors, so we'll need to flatten the images\n",
        "# Convert images to numpy arrays for SMOTE\n",
        "train_images = [img.numpy().flatten() for img, _ in train_dataset]\n",
        "train_labels = [label for _, label in train_dataset]\n",
        "smote = SMOTE(random_state=42)\n",
        "train_images_resampled, train_labels_resampled = smote.fit_resample(train_images, train_labels)\n",
        "train_images_resampled = torch.Tensor(np.array(train_images_resampled)).view(-1, 3, 32, 32)\n",
        "train_labels_resampled = torch.LongTensor(train_labels_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8dSaq9OpqlVo"
      },
      "outputs": [],
      "source": [
        "# # Store flattened vectors for train data\n",
        "# X_train = train_images_resampled.view(-1, 28 * 28)\n",
        "# Y_train = train_labels_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5KqQwS3qlVo"
      },
      "source": [
        "### Step 4: Build CNN & Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbqisOH8qlVo",
        "outputId": "ef724a40-c676-4c87-e719-2162203e7f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the CNN ... \n",
            "Epoch 1/10, Loss: 0.681249429860115\n",
            "Epoch 2/10, Loss: 0.25750763940811155\n",
            "Epoch 3/10, Loss: 0.14672491289377212\n",
            "Epoch 4/10, Loss: 0.09565989250063896\n",
            "Epoch 5/10, Loss: 0.06459150330781936\n",
            "Epoch 6/10, Loss: 0.04818558646805585\n",
            "Epoch 7/10, Loss: 0.03930638539776206\n",
            "Epoch 8/10, Loss: 0.03287420496612787\n",
            "Epoch 9/10, Loss: 0.029538248270601034\n",
            "Epoch 10/10, Loss: 0.017137879557819105\n"
          ]
        }
      ],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 64 * 6 * 6)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits, x  # Return logits and features from the penultimate layer\n",
        "\n",
        "cnn = CNN()\n",
        "# Define optimizer and loss function\n",
        "optimizer_cnn = optim.Adam(cnn.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Training the CNN ... \")\n",
        "\n",
        "# Train the CNN\n",
        "def train_cnn(model, train_loader, optimizer, criterion, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(data)\n",
        "            loss = criterion(logits, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader.dataset)}\")\n",
        "\n",
        "train_loader_cnn = DataLoader(list(zip(train_images_resampled, train_labels_resampled)), batch_size=64, shuffle=True)\n",
        "train_cnn(cnn, train_loader_cnn, optimizer_cnn, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fbJpGJuUqlVp"
      },
      "outputs": [],
      "source": [
        "# # Store flattened vectors for test data\n",
        "# X_test = test_images.view(-1, 28 * 28)\n",
        "# Y_test = test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CoahddBqlVp"
      },
      "source": [
        "### Step 5: Build a Multi-layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJmDVtsyqlVp",
        "outputId": "0a949d15-cd45-443a-f275-dcce1732b831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the MLP ... \n"
          ]
        }
      ],
      "source": [
        "#Extract features from the CNN to feed into the MLP\n",
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    train_features = []\n",
        "    for data, _ in train_loader_cnn:\n",
        "        _, features = cnn(data)\n",
        "        train_features.append(features)\n",
        "    train_features = torch.cat(train_features)\n",
        "\n",
        "#Build a Multi-layer Perceptron\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp = MLP()\n",
        "# Define optimizer and loss function\n",
        "optimizer_mlp = optim.Adam(mlp.parameters(), lr=0.001)\n",
        "criterion_mlp = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Training the MLP ... \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAL8uFYDqlVq"
      },
      "source": [
        "### Step 6: Apply cross-entropy loss for loss calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzYt7c9cqlVq",
        "outputId": "b393d0f8-6131-4ed4-c943-1040cbde8a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.308327042312622\n",
            "Epoch 2/10, Loss: 2.302695669631958\n",
            "Epoch 3/10, Loss: 2.302870717163086\n",
            "Epoch 4/10, Loss: 2.301990905075073\n",
            "Epoch 5/10, Loss: 2.301452420425415\n",
            "Epoch 6/10, Loss: 2.3004353280639647\n",
            "Epoch 7/10, Loss: 2.2984436138916013\n",
            "Epoch 8/10, Loss: 2.2964931101226806\n",
            "Epoch 9/10, Loss: 2.292941941986084\n",
            "Epoch 10/10, Loss: 2.288054637680054\n"
          ]
        }
      ],
      "source": [
        "# Training the MLP\n",
        "def train_mlp(model, train_loader, optimizer, criterion, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader.dataset)}\")\n",
        "\n",
        "train_loader_mlp = DataLoader(list(zip(train_features, train_labels_resampled)), batch_size=64, shuffle=True)\n",
        "train_mlp(mlp, train_loader_mlp, optimizer_mlp, criterion_mlp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz-yls_jqlVq"
      },
      "source": [
        "### Step 8: Calculate final accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs7uVZ-CqlVq",
        "outputId": "1319db8b-2d2b-48ea-d886-6981957491a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Test Accuracy: 9.2\n"
          ]
        }
      ],
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "test_loader_cnn = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "test_features = []\n",
        "test_labels = []\n",
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader_cnn:\n",
        "        _, features = cnn(data)\n",
        "        test_features.append(features)\n",
        "        test_labels.append(labels)\n",
        "test_features = torch.cat(test_features)\n",
        "test_labels = torch.cat(test_labels)\n",
        "\n",
        "test_loader_mlp = DataLoader(list(zip(test_features, test_labels)), batch_size=64, shuffle=False)\n",
        "accuracy = test(mlp, test_loader_mlp)\n",
        "\n",
        "print(f\"MLP Test Accuracy: {accuracy*100}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}